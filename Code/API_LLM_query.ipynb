{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abb76606-11a6-4b4c-87b8-c8da262d6d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# QA on EES API data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e4d3a62-0217-4a85-bd23-21ee70809006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code is used to do QA checks on data which has been published on the EES API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ab37b8-160c-408f-a897-f55d2f59c888",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ead1403-a5d4-4d02-ad99-0fc7e026210e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inputs"
    }
   },
   "outputs": [],
   "source": [
    "API_URL = \"https://api.education.gov.uk/statistics/v1/data-sets/1d419801-435d-c676-b428-1217e08290c3/csv?dataSetVersion=1.0\"\n",
    "numeric_cols = [\"achievements_percent\", \"starts_percent\", \"participation_count\", \"achievement_count\", \"start_count\"]\n",
    "output_name = \"app headlines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4ceb72-731a-4ac8-b6a0-36b39157f0ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create prompt to QA EES API"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    " \n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=\"https://adb-2220072380334347.7.azuredatabricks.net/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Ask the API to write code which will QA the data on the API in the input\n",
    "response = client.chat.completions.create(\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are an analyst\"\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"write Python code to read in and quality assure the data on this API: {API_URL}. Please check for outliers,  missing values, and data type. Please also present summary statistics on the data. Do not include any backticks.\n",
    "            \n",
    "            Please convert any columns mentioned in {numeric_cols} to numeric type using pandas. This is very important.\n",
    "\n",
    "            Please output **only** valid, bug-free Python code. Ensure that:\n",
    "            1. The code is properly indented.\n",
    "            2. Do not include any markdown or formatting characters like backticks (`), triple quotes (```), or explanations.\n",
    "            3. The code should be plain Python without any comments, explanations, or descriptions.\n",
    "            4. Ensure the code is formatted to be executable without further modifications, with the correct indentation maintained throughout.\n",
    "            5. Print all the outputs at the end, along with an explanation of what they are.\n",
    "            6. Save the outputs in a text file in the Outputs folder which is stored a level above this script, and include {output_name} in the filename.\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=8000\n",
    ")\n",
    "\n",
    "generated_code = response.choices[0].message.content\n",
    "\n",
    "# Run the code the LLM generated\n",
    "exec(generated_code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eac731b-7eec-4aaf-af7c-80660fd6468f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62fb69c-5d70-44c4-bd0e-ecc8f51d0598",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model comparison"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_opts = [\"databricks-gpt-oss-120b\", \"databricks-gpt-oss-20b\", \"databricks-claude-3-7-sonnet\", \"databricks-claude-sonnet-4\", \"databricks-llama-4-maverick\", \"databricks-gemma-3-12b\", \"databricks-meta-llama-3-3-70b-instruct\", \"databricks-meta-llama-3-1-8b-instruct\"]\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    " \n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=\"https://adb-2220072380334347.7.azuredatabricks.net/serving-endpoints\"\n",
    ")\n",
    "\n",
    "for model in model_opts:\n",
    "    print(f\"Testing model {model}\")\n",
    "    # Ask the API to write code which will QA the data on the API in the input\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"databricks-llama-4-maverick\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are an analyst\"\n",
    "            },\n",
    "\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"write Python code to read in and quality assure the data on this API: {API_URL}. Please check for outliers,  missing values, and data type. Please also present summary statistics on the data. Do not include any backticks.\n",
    "            \n",
    "                Please convert any columns mentioned in {numeric_cols} to numeric type using pandas. This is very important.\n",
    "\n",
    "                Please output **only** valid, bug-free Python code. Ensure that:\n",
    "                1. The code is properly indented.\n",
    "                2. Do not include any markdown or formatting characters like backticks (`), triple quotes (```), or explanations.\n",
    "                3. The code should be plain Python without any comments, explanations, or descriptions.\n",
    "                4. Ensure the code is formatted to be executable without further modifications, with the correct indentation maintained throughout.\n",
    "                5. Print all the outputs at the end, along with an explanation of what they are.\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=8000\n",
    "    )\n",
    "\n",
    "    generated_code = response.choices[0].message.content.replace(\"`\", \"\").replace(\"python\", \"\")\n",
    "\n",
    "    try:\n",
    "        # Run the code the LLM generated\n",
    "        exec(generated_code, globals())\n",
    "        print(f\"Model {model} passed!\")\n",
    "\n",
    "    except (SyntaxError, TypeError) as e:\n",
    "        print(f\"Model {model} failed due to {e}.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985671b8-ad1b-4ee6-a6d7-ec32ebdab64f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = 'https://api.education.gov.uk/statistics/v1/data-sets/1d419801-435d-c676-b428-1217e08290c3/csv?dataSetVersion=1.0'\n",
    "urlretrieve(url, 'data.csv')\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "cols_to_convert = ['achievements_percent', 'starts_percent', 'participation_count', 'achievement_count', 'start_count']\n",
    "for col in cols_to_convert:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print('Data Types:')\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "print('Summary Statistics:')\n",
    "print(df.describe())\n",
    "print()\n",
    "\n",
    "print('Missing Values Count:')\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print('Outliers Count:')\n",
    "print(((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "API_LLM_query",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
