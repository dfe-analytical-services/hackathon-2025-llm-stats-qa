{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6bea8b1-a8f4-4429-a6e9-218ea336a7ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary of data in csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5ea3f67-ada6-4eec-9d50-11d1558abb5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "eea04406-44cb-4b38-9d51-9851990b6964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install openai\n",
    "%pip install markdown\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd55fd8-5b76-4216-b93d-160c2e6c2fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b533486-99f1-4561-99db-2fdc7ca8047f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read in data and metadata from the publication of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127e29a5-347b-44e8-83a9-7c33c4b7691a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "national_char_data = pd.read_csv(\"data/202324_national_char_data_revised.csv\")\n",
    "national_char_metadata = pd.read_csv(\"data/202324_national_char_data_revised.meta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59d3aeb1-7a1e-439e-a438-60910a168a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create JSONs for each characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479059e2-dcf2-4715-90e7-d689f748096f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for All state-funded schools' rows only\n",
    "national_char_data_all_statefunded = national_char_data[national_char_data['establishment_type_group'] == 'All state-funded']\n",
    "\n",
    "national_char_data_measures = national_char_data_all_statefunded[['time_period', 'sex', 'ethnicity_major', 'ethnicity_minor', 'free_school_meals', 'sen_provision', 'sen_status', 'sen_primary_need', 'disadvantage', 'first_language', 'religious_denomination', 'admission_type', 't_pupils', 'avg_att8', 'pt_l2basics_95', 'avg_p8score', 'pt_ebacc_e_ptq_ee', 'avg_ebaccaps']]\n",
    "\n",
    "# Create the new 'ethnicity' column\n",
    "national_char_data_measures['ethnicity'] = national_char_data_measures.apply(\n",
    "    lambda row: row['ethnicity_minor'] if row['ethnicity_minor'] != 'Total' else row['ethnicity_major'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Modify the 'ethnicity_major' column to only be not 'Total' if 'ethnicity_minor' is 'Total'\n",
    "national_char_data_measures['ethnicity_major'] = national_char_data_measures.apply(\n",
    "    lambda row: row['ethnicity_major'] if row['ethnicity_minor'] == 'Total' else 'Total',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# List of columns to iterate over\n",
    "columns_to_vary = [\n",
    "    'sex', 'ethnicity_major', 'ethnicity_minor', 'free_school_meals',\n",
    "    'sen_provision', 'sen_status', 'sen_primary_need',\n",
    "    'disadvantage','first_language', 'admission_type', 'religious_denomination'\n",
    "]\n",
    "\n",
    "# Base filter\n",
    "base_filter = {\n",
    "    'sex': 'Total',\n",
    "    'ethnicity_major': 'Total',\n",
    "    'ethnicity_minor': 'Total',\n",
    "    'admission_type': 'Total',\n",
    "    'sen_provision': 'Total',\n",
    "    'sen_primary_need': 'Total',\n",
    "    'sen_status': 'Total',\n",
    "    'disadvantage': 'Total',\n",
    "    'free_school_meals': 'Total',\n",
    "    'first_language': 'Total',\n",
    "    'religious_denomination': 'Total'\n",
    "}\n",
    "\n",
    "# Store results\n",
    "json_outputs = {}\n",
    "\n",
    "for col in columns_to_vary:\n",
    "    # Create a copy of the base filter\n",
    "    filter_conditions = base_filter.copy()\n",
    "\n",
    "    # Modify the current column to != 'Total'\n",
    "    filtered_data = national_char_data_measures[\n",
    "        pd.concat([\n",
    "            national_char_data_measures[c] == v if c != col else national_char_data_measures[c] != 'Total'\n",
    "            for c, v in filter_conditions.items()\n",
    "        ], axis=1).all(axis=1)\n",
    "    ]\n",
    "\n",
    "    # Convert to JSON and store\n",
    "    json_outputs[col] = filtered_data.to_json(orient='records')\n",
    "\n",
    "# Convert the metadata to JSON for the prompt\n",
    "json_metadata_for_prompt = national_char_metadata.to_json(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01ad05a-372d-408f-901e-995ee0156ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb70f77-b9a4-4264-9ec2-29b95ea0eb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the API key from Databricks secrets\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=\"https://adb-2220072380334347.7.azuredatabricks.net/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# What indicators are the focus?\n",
    "performance_measures = [\n",
    "    \"pt_ebacc_e_ptq_ee\",\n",
    "    \"pt_ebacc_94\",\n",
    "    \"pt_ebacc_95\",\n",
    "    \"avg_ebaccaps\"\n",
    "]\n",
    "\n",
    "system_content = \"You are creating narrative sections around specific policy areas based on the data provided. The narrative is a national statistics, statistical release so it has to be very matter-of-fact. \"\n",
    "\n",
    "# List of characteristics to loop through\n",
    "characteristics = [\n",
    "    'sex', 'ethnicity_major', 'ethnicity_minor',\n",
    "    'admission_type', 'sen_provision', 'sen_primary_need',\n",
    "    'sen_status', 'disadvantage', 'free_school_meals',\n",
    "    'first_language', 'religious_denomination'\n",
    "]\n",
    "\n",
    "# Store markdown sections\n",
    "markdown_sections = []\n",
    "\n",
    "for characteristic in characteristics:\n",
    "    # Accordion title\n",
    "    accordion_title = f\"### EBacc entry and achievement by {characteristic.replace('_', ' ').title()}\\n\"\n",
    "\n",
    "    # Prompt for the model\n",
    "    prompt = f\"\"\"\n",
    "Here is the data:\n",
    "{json_outputs[characteristic]}\n",
    "Here is the metadata: \n",
    "{json_metadata_for_prompt}\n",
    "The indicators of interest are:\n",
    "{performance_measures}\n",
    "Create a summary for {characteristic} featuring tables and a short narrative summary based on the tables. The summary needs to fit under the heading {accordion_title.strip()}. For each indicator of interest available for {characteristic} please create one summary table using the unique values under {characteristic} as rows and time_period as columns. Calculate the gaps in the indicators across the unique values in the {characteristic} column and produce separate tables for gaps (these should only be rendered where there are less than 6 unique values, and no gaps tabled should be rendered for ethnicity_minor or religious_denomination). time_period specifies the academic year (for example 202324 is 2023/24). time_period should be in the order 202324, 202223, 202122, 202021, 201920, 201819. Values should remain with the time_period they relate to. Use the metadata to understand what the indicators are and name the table according to the metadata information on the indicator used. Don't guess what the indicators are, the metadata will tell you. If an indicator of interest is not available at all for {characteristic} do not attempt to produce a table. Tables should follow the order pt_ebacc_e_ptq_ee, pt_ebacc_94 pt_ebacc_95,avg_ebaccaps, then tables for the gaps in each of these indicators in the same order. Do not produce a blank table if an indicator is unavailable. If any indicators do not have complete timeseries, it is okay to just focus on the years where these indicators are available for those indicators. Please provide a short narrative summary based on only the information in the tables. \n",
    "\"\"\"\n",
    "\n",
    "    # Send to LLM\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_content\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.01,\n",
    "    max_tokens=5000\n",
    "    )   \n",
    "\n",
    "    # Append the markdown section\n",
    "    markdown_sections.append(accordion_title + response.choices[0].message.content + \"\\n\\n\")\n",
    "\n",
    "# Combine all sections into one markdown document\n",
    "final_markdown = \"# EBacc Entry and Achievement Summary\\n\\n\" + \"\\n\".join(markdown_sections)\n",
    "\n",
    "# Optionally print or save\n",
    "print(final_markdown)\n",
    "\n",
    "# To save to a file:\n",
    "with open(\"ebacc_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_markdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b445c3a3-9811-4d86-81ab-3dcce3a4a584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feed report back into LLM for editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03b5654-5d2d-4afb-b7df-9bdf1dd15b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the API key from Databricks secrets\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=\"https://adb-2220072380334347.7.azuredatabricks.net/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Read the Markdown file (or change to .html if preferred)\n",
    "with open(\"ebacc_summary.md\", \"r\", encoding=\"utf-8\") as md_file:\n",
    "    txt_content = md_file.read()\n",
    "\n",
    "# Define a new system context\n",
    "system_content = \"You an expert editor who specialises in summarising reports for national statistics.\"\n",
    "\n",
    "# Define the prompt\n",
    "prompt = f\"\"\"\n",
    "Please:\n",
    "- Remove redundant content across the subsections\n",
    "- Remove unnecessary information, e.g., about things that are not available\n",
    "- Do not lose the messages about the characteristics and gaps in indicators in the sub sections.\n",
    "- Do not lose subsections.\n",
    "- Do not lose the timeseries information.\n",
    "- Create a bullet-pointed summary at the top of the report that summarises any key points.\n",
    "- Improve the clarity and flow across sections.\n",
    "Here is the report:\n",
    "{txt_content}\n",
    "\"\"\"\n",
    "\n",
    "# Send to LLM\n",
    "# \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "# \"databricks-llama-4-maverick\"\n",
    "response = client.chat.completions.create(\n",
    "model=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_content\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "],\n",
    "temperature=0.01,\n",
    "max_tokens=5000\n",
    ")   \n",
    "\n",
    "# Append the markdown section\n",
    "cleaned_summary = response.choices[0].message.content\n",
    "\n",
    "# Save the edited report\n",
    "with open(\"ebacc_summary_cleaned.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc362e3-10ea-4843-b67a-6f49de902de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Convert .md to a .html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a644c4-5586-4702-b286-4eff45cb0b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the Markdown file\n",
    "with open(\"ebacc_summary_cleaned.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "# Convert Markdown to HTML\n",
    "html_content = markdown.markdown(markdown_content, extensions=['tables'])\n",
    "\n",
    "# Save the HTML content to a file\n",
    "with open(\"ebacc_summary_cleaned.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_content)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_narrative_for_characteristics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
